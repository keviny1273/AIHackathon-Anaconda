{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Wine Quality Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this little example we will look at several ways to predict the quality of wine based on several measurable quanities. But remember, waine tasting is largely a matter of personal taste.\n",
    "\n",
    "Frist, let's invoke some of the imports we will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import urllib\n",
    "import random\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import tarfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to make sure we have the data sets we needed downloaded. First let's get our data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "white_wine_file = \"winequality-white.csv\"\n",
    "if not os.path.exists(white_wine_file):\n",
    "    urllib.request.urlretrieve(\"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv\", white_wine_file)\n",
    "\n",
    "red_wine_file = \"winequality-red.csv\"\n",
    "if not os.path.exists(red_wine_file):\n",
    "    urllib.request.urlretrieve(\"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\", red_wine_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to load and explore the data set. Load them into memory using the numpy tooling. The columns have the following meaings:\n",
    "'fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol', 'quality'. We will pretty much ignore these as we will be doing ML based only on non-expert traning straegies. We also separate out the labels from the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "White wine features:\nHistogram of labels (w) =  [   0    0    0   20  163 1457 2198  880  175    5    0]\nFeature =        fixed acidity  (min =  3.800, ave =  6.855, max = 14.200).\nFeature =     volatile acidity  (min =  0.080, ave =  0.278, max =  1.100).\nFeature =          citric acid  (min =  0.000, ave =  0.334, max =  1.660).\nFeature =       residual sugar  (min =  0.600, ave =  6.391, max = 65.800).\nFeature =            chlorides  (min =  0.009, ave =  0.046, max =  0.346).\nFeature =  free sulfur dioxide  (min =  2.000, ave = 35.308, max = 289.000).\nFeature = total sulfur dioxide  (min =  9.000, ave = 138.361, max = 440.000).\nFeature =              density  (min =  0.987, ave =  0.994, max =  1.039).\nFeature =                   pH  (min =  2.720, ave =  3.188, max =  3.820).\nFeature =            sulphates  (min =  0.220, ave =  0.490, max =  1.080).\nFeature =              alcohol  (min =  8.000, ave = 10.514, max = 14.200).\n\nRed wine features:\nHistogram of labels (r) =  [  0   0   0  10  53 681 638 199  18   0   0]\nFeature =        fixed acidity  (min =  4.600, ave =  8.320, max = 15.900).\nFeature =     volatile acidity  (min =  0.120, ave =  0.528, max =  1.580).\nFeature =          citric acid  (min =  0.000, ave =  0.271, max =  1.000).\nFeature =       residual sugar  (min =  0.900, ave =  2.539, max = 15.500).\nFeature =            chlorides  (min =  0.012, ave =  0.087, max =  0.611).\nFeature =  free sulfur dioxide  (min =  1.000, ave = 15.875, max = 72.000).\nFeature = total sulfur dioxide  (min =  6.000, ave = 46.468, max = 289.000).\nFeature =              density  (min =  0.990, ave =  0.997, max =  1.004).\nFeature =                   pH  (min =  2.740, ave =  3.311, max =  4.010).\nFeature =            sulphates  (min =  0.330, ave =  0.658, max =  2.000).\nFeature =              alcohol  (min =  8.400, ave = 10.423, max = 14.900).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tags = np.array(['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol'])\n",
    "ww = np.loadtxt(white_wine_file, skiprows=1, delimiter=';')\n",
    "ww_labels = ww[:, 11]\n",
    "ww_features = ww[:, range(11)]\n",
    "rw = np.loadtxt(red_wine_file, skiprows=1, delimiter=';')\n",
    "rw_labels = rw[:, 11]\n",
    "rw_features = rw[:, range(11)]\n",
    "\n",
    "w_hist, _ = np.histogram(ww_labels, [0,1,2,3,4,5,6,7,8,9,10,11])\n",
    "r_hist, _ = np.histogram(rw_labels, [0,1,2,3,4,5,6,7,8,9,10,11])\n",
    "\n",
    "print(\"White wine features:\")\n",
    "print(\"Histogram of labels (w) = \", w_hist)\n",
    "for w in range(11):\n",
    "    print(\"Feature = %20s  (min = %6.3f, ave = %6.3f, max = %6.3f).\" % (tags[w], np.min(ww_features[:,w]), np.average(ww_features[:,w]), np.max(ww_features[:,w])))\n",
    "\n",
    "print(\"\\nRed wine features:\")\n",
    "print(\"Histogram of labels (r) = \", r_hist)\n",
    "for w in range(11):\n",
    "    print(\"Feature = %20s  (min = %6.3f, ave = %6.3f, max = %6.3f).\" % (tags[w], np.min(rw_features[:,w]), np.average(rw_features[:,w]), np.max(rw_features[:,w])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to divide our data into training, validation, and test data sets. Typically we target 80, 10, 10. Just in case the existing data has some existing assumptions about the order, we will take random samples or each data set. Notice that we explicitly set the random number generator seed. This way we get the same partitioning everytime we rerun the program. \n",
    "\n",
    "Given the histograms above, we will combine the classes for the first 5 classes into on and the last 3 in to 1.\n",
    "\n",
    "From this point on we will focus on the white wine only. The red wine is left to the reader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wn = len(ww_features)\n",
    "random.seed(26)\n",
    "\n",
    "# Select 3 or 5 classes. Notice that when changing the number of classes, you need to delete the temporary\n",
    "# directories and their contents, e.g. bottleneck, retain_logs, SavedFeatures, wines, wines_te, wines_tr, wines_va.\n",
    "n_classes = 3\n",
    "if n_classes == 5:\n",
    "    label_map = [0, 0, 0, 0, 0, 1, 2, 3, 4, 4, 4]\n",
    "if n_classes == 3:\n",
    "    label_map = [0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2]\n",
    "\n",
    "ww_ind = random.sample(range(wn), wn)\n",
    "ww_f_tr = np.array([ww_features[ww_ind[i]] for i in range(0, int(0.8*wn))])\n",
    "ww_f_va = np.array([ww_features[ww_ind[i]] for i in range(int(0.8*wn)+1, int(0.9*wn))])\n",
    "ww_f_te = np.array([ww_features[ww_ind[i]] for i in range(int(0.9*wn)+1, wn-1)])\n",
    "ww_l_tr = np.array([label_map[int(ww_labels[ww_ind[i]])] for i in range(0, int(0.8*wn))])\n",
    "ww_l_va = np.array([label_map[int(ww_labels[ww_ind[i]])] for i in range(int(0.8*wn)+1, int(0.9*wn))])\n",
    "ww_l_te = np.array([label_map[int(ww_labels[ww_ind[i]])] for i in range(int(0.9*wn)+1, wn-1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K Nearest Neighbour Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with a simple K Nearest Neighbours (KNN) style machine learning. This is a straingt forwards process requiring only a single traing step. Notice that we only list the results for the validation set. This is because we will be tuning the KNN parameters and don't want to over fit against the testing data set. To train the KNN classifier you can change the numbr of neighboours and the weighting strategy ('uniform' or 'distance')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction = 1 Actual = 1 Probabilities = 0.199 0.801 0.000\nPrediction = 2 Actual = 2 Probabilities = 0.276 0.291 0.433\nPrediction = 0 Actual = 0 Probabilities = 0.639 0.361 0.000\nPrediction = 1 Actual = 1 Probabilities = 0.000 1.000 0.000\nPrediction = 1 Actual = 2 Probabilities = 0.359 0.411 0.230\nPrediction = 1 Actual = 1 Probabilities = 0.000 1.000 0.000\nPrediction = 2 Actual = 2 Probabilities = 0.000 0.000 1.000\nPrediction = 1 Actual = 1 Probabilities = 0.000 1.000 0.000\nPrediction = 1 Actual = 1 Probabilities = 0.205 0.547 0.247\nPrediction = 1 Actual = 1 Probabilities = 0.000 0.804 0.196\n\n Overall score = 63.60%\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=5, weights='distance')\n",
    "knn.fit(ww_f_tr, ww_l_tr)\n",
    "\n",
    "# Look at 10 of our validation set:\n",
    "for i in range(10):\n",
    "    pred = knn.predict([ww_f_va[i]])[0]\n",
    "    probs = knn.predict_proba([ww_f_va[i]])\n",
    "    if n_classes == 5:\n",
    "        print (\"Prediction = %1d Actual = %1d Probabilities = %5.3f %5.3f %5.3f %5.3f %5.3f\" % \n",
    "            (pred, ww_l_va[i], probs[0][0], probs[0][1], probs[0][2], probs[0][3], probs[0][4]))\n",
    "    if n_classes == 3:\n",
    "        print (\"Prediction = %1d Actual = %1d Probabilities = %5.3f %5.3f %5.3f\" % \n",
    "            (pred, ww_l_va[i], probs[0][0], probs[0][1], probs[0][2]))\n",
    "    \n",
    "# Run the complete validation data set.\n",
    "score = knn.score(ww_f_va, ww_l_va)\n",
    "print(\"\\n Overall score = %5.2f%%\" % (100.0*score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets have a closer look at how our system in performaing. We'll look at the false positives and false negatives for each class. False positive is where the class was incorrectly predicted; a false negative is where a wrong class was predicited. In our case we will have a false positive somewhere for every false negative, but any patterns in the distribution could be interesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N =  489\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total wrong = 178. Eror rate =  36.4008%\nClass =  0 False positive =  48 (  9.8%) False negative =  57 ( 11.7%).\nClass =  1 False positive =  85 ( 17.4%) False negative =  72 ( 14.7%).\nClass =  2 False positive =  45 (  9.2%) False negative =  49 ( 10.0%).\n"
     ]
    }
   ],
   "source": [
    "fp = np.zeros(n_classes)\n",
    "fn = np.zeros(n_classes)\n",
    "n = len(ww_f_va)\n",
    "tot = 0\n",
    "print(\"N = \", n)\n",
    "for i in range(n):\n",
    "    pred = knn.predict([ww_f_va[i]])[0]\n",
    "    if pred != ww_l_va[i]:\n",
    "        tot = tot + 1\n",
    "        fn[int(ww_l_va[i])] = fn[int(ww_l_va[i])] + 1\n",
    "        fp[int(pred)] = fp[int(pred)] + 1\n",
    "\n",
    "print(\"Total wrong = %3d. Eror rate = %8.4f%%\" % (tot, 100.0*tot/n))\n",
    "for i in range(n_classes):\n",
    "    print(\"Class = %2d False positive = %3d (%5.1f%%) False negative = %3d (%5.1f%%).\" % (i, fp[i], 100.0*fp[i]/n, fn[i\n",
    "    ], 100.0*fn[i]/n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The low false positives for the best and worst wines are encoraging. This means we are unlikely to be told a wine is good (or bad) incorrectly. However, we miss the oppertunity to sample about 4% of the best wines (false negatives), but these numbers too are relatively small.\n",
    "\n",
    "Observing this ability to identify good, bad, and ok wines, the reader might wish to try further restricting the number of classes and see how well the classifier functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Imagification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic idea behine 'imigification' is that without needing to understand the detailed meaning of feature data we can still gain an insight into how well we might be able to classify this data. In the case of wine we were able to do pretty well with just using the KNN classifier, but in some cases there will be many more features or simply large volumes of data associated with each instance. In this case we want to develop a way to classify these without a detailed understanding of the specialist knowledge associated with the data.\n",
    "\n",
    "We have also discovered that modern image perception networks are quite good with detail that sometimes humans miss. We would like to exploit this learning with a technique called transfer learning. So while the wine case may not seem to demand this approach, we'll have a go anyway to see how straight forward the process of imagification can be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capturing the Data as Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in the process is to capture the data as images. Let's try a bar chart. [Note that we are normalizing all the features against the average across the whole data set so that all bars have about the same impact in the image.]Let's look at just one instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x209cb8c39e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.close()\n",
    "fig, bar = plt.subplots()\n",
    "indx = range(11)\n",
    "p = 50\n",
    "features = [ww_f_tr[p][f]/np.average(ww_f_tr[:,f]) for f in range(11)]\n",
    "bar.bar(indx, features)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to build the image data set to use in training the image recognizer. We will create three directories to stor the images, one each for training, validation, and testing. This step will take some time. Go have a coffee."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(\"wines_tr\"):\n",
    "    os.makedirs(\"wines_tr\")\n",
    "\n",
    "    f_inds = range(11)\n",
    "    for p in range (len(ww_f_tr)):\n",
    "        features = ww_f_tr[p]\n",
    "        for f in f_inds:\n",
    "            features[f] = features[f]/np.average(ww_f_tr[:,f])\n",
    "            \n",
    "        plt.close()\n",
    "        _, bar = plt.subplots()\n",
    "        bar.bar(f_inds, features)\n",
    "        plt.savefig(\"wines_tr/WW%04d.jpeg\"%(p))\n",
    "\n",
    "if not os.path.exists(\"wines_va\"):\n",
    "    os.makedirs(\"wines_va\")\n",
    "\n",
    "    f_inds = range(11)\n",
    "    for p in range (len(ww_f_va)):\n",
    "        features = ww_f_va[p]\n",
    "        for f in f_inds:\n",
    "            features[f] = features[f]/np.average(ww_f_tr[:,f])\n",
    "            \n",
    "        plt.close()\n",
    "        _, bar = plt.subplots()\n",
    "        bar.bar(f_inds, features)\n",
    "        plt.savefig(\"wines_va/WW%04d.jpeg\"%(p))\n",
    "\n",
    "if not os.path.exists(\"wines_te\"):\n",
    "    os.makedirs(\"wines_te\")\n",
    "\n",
    "    f_inds = range(11)\n",
    "    for p in range (len(ww_f_te)):\n",
    "        features = ww_f_te[p]\n",
    "        for f in f_inds:\n",
    "            features[f] = features[f]/np.average(ww_f_tr[:,f])\n",
    "            \n",
    "        plt.close()\n",
    "        _, bar = plt.subplots()\n",
    "        bar.bar(f_inds, features)\n",
    "        plt.savefig(\"wines_te/WW%04d.jpeg\"%(p))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As simple approach to imagification and transfer learing is to remove the final layer of an existing image recognizer and replace it with a KNN classifier using the final pooling layer are the source for our features. This easily demonstrates the ability to adapt CNNs for use in transfer learning. The success of ths approach depends on how well our images capture the essential infomration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First make sure the Inception and TensorFlow environment is set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(\"model\"):\n",
    "    os.makedirs(\"model\")\n",
    "    \n",
    "if not os.path.exists(\"model/inception-2015-12-05.tgz\"):\n",
    "    filepath, _ = urllib.request.urlretrieve(\n",
    "            \"http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz\", \"model/inception-2015-12-05.tgz\")\n",
    "    tarfile.open(filepath, 'r:gz').extractall(\"model\")\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to run all our images through the Inception CNN and then set up a KNN to train against the Known labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Inception CNN.\n",
    "tf.reset_default_graph()\n",
    "f = tf.gfile.FastGFile(\"model/classify_image_graph_def.pb\", 'rb')\n",
    "graph_def = tf.GraphDef()\n",
    "graph_def.ParseFromString(f.read())\n",
    "tf.import_graph_def(graph_def, name='')\n",
    "sess = tf.Session()\n",
    "\n",
    "if os.path.exists (\"SavedFeatures/tr_f.npy\"):\n",
    "\n",
    "    tr_features = np.load(\"SavedFeatures/tr_f.npy\")\n",
    "    tr_labels = np.load(\"SavedFeatures/tr_l.npy\")\n",
    "    va_features = np.load(\"SavedFeatures/va_f.npy\")\n",
    "    va_labels = np.load(\"SavedFeatures/va_l.npy\")\n",
    "    te_features = np.load(\"SavedFeatures/te_f.npy\")\n",
    "    te_labels = np.load(\"SavedFeatures/te_l.npy\")\n",
    "\n",
    "else:\n",
    "\n",
    "    os.makedirs(\"SavedFeatures\")\n",
    "    \n",
    "    pool3 = sess.graph.get_tensor_by_name('pool_3:0')\n",
    "\n",
    "    tr_features = np.empty((len(ww_f_tr), 2048), dtype='float32')\n",
    "    tr_labels = np.empty(len(ww_l_tr), dtype='int')\n",
    "    for i in range(len(ww_f_tr)):\n",
    "        image_f = \"wines_tr/WW%04d.jpeg\" % (i)\n",
    "        image = tf.gfile.FastGFile(image_f, 'rb').read()\n",
    "        tr_features[i] = sess.run(pool3, {'DecodeJpeg/contents:0': image})[0][0][0]\n",
    "        tr_labels[i] = ww_l_tr[i]\n",
    "        if i % 100 == 0:\n",
    "            print(\"Generating training feature set for image number = \", i)\n",
    "\n",
    "    va_features = np.empty((len(ww_f_va), 2048), dtype='float32')\n",
    "    va_labels = np.empty(len(ww_l_va), dtype='int')\n",
    "    for i in range(len(ww_f_va)):\n",
    "        image_f = \"wines_va/WW%04d.jpeg\" % (i)\n",
    "        image = tf.gfile.FastGFile(image_f, 'rb').read()\n",
    "        va_features[i] = sess.run(pool3, {'DecodeJpeg/contents:0': image})[0][0][0]\n",
    "        va_labels[i] = ww_l_va[i]\n",
    "        if i % 100 == 0:\n",
    "            print(\"Generating validation feature set for image number = \", i)\n",
    "            \n",
    "    te_features = np.empty((len(ww_f_te), 2048), dtype='float32')\n",
    "    te_labels = np.empty(len(ww_l_te), dtype='int')\n",
    "    for i in range(len(ww_f_te)):\n",
    "        image_f = \"wines_te/WW%04d.jpeg\" % (i)\n",
    "        image = tf.gfile.FastGFile(image_f, 'rb').read()\n",
    "        te_features[i] = sess.run(pool3, {'DecodeJpeg/contents:0': image})[0][0][0]\n",
    "        te_labels[i] = ww_l_te[i]\n",
    "        if i % 100 == 0:\n",
    "            print(\"Generating ttest feature set for image number = \", i)\n",
    "\n",
    "    np.save(\"SavedFeatures/tr_f.npy\", tr_features)\n",
    "    np.save(\"SavedFeatures/tr_l.npy\", tr_labels)\n",
    "    np.save(\"SavedFeatures/va_f.npy\", va_features)\n",
    "    np.save(\"SavedFeatures/va_l.npy\", va_labels)\n",
    "    np.save(\"SavedFeatures/te_f.npy\", te_features)\n",
    "    np.save(\"SavedFeatures/te_l.npy\", te_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train and validate our KNN based on the features extracted from the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction = 0 Actual = 1 Probabilities = 0.432 0.381 0.187\n",
      "Prediction = 1 Actual = 2 Probabilities = 0.000 0.812 0.188\n",
      "Prediction = 1 Actual = 0 Probabilities = 0.184 0.816 0.000\n",
      "Prediction = 1 Actual = 1 Probabilities = 0.398 0.602 0.000\n",
      "Prediction = 1 Actual = 2 Probabilities = 0.000 0.826 0.174\n",
      "Prediction = 1 Actual = 1 Probabilities = 0.210 0.593 0.198\n",
      "Prediction = 1 Actual = 2 Probabilities = 0.184 0.639 0.177\n",
      "Prediction = 1 Actual = 1 Probabilities = 0.229 0.771 0.000\n",
      "Prediction = 0 Actual = 1 Probabilities = 0.442 0.371 0.187\n",
      "Prediction = 2 Actual = 1 Probabilities = 0.201 0.397 0.402\n",
      "\n",
      " Overall score = 39.47%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction = 0 Actual = 1 Probabilities = 0.432 0.381 0.187\nPrediction = 1 Actual = 2 Probabilities = 0.000 0.812 0.188\nPrediction = 1 Actual = 0 Probabilities = 0.184 0.816 0.000\nPrediction = 1 Actual = 1 Probabilities = 0.398 0.602 0.000\nPrediction = 1 Actual = 2 Probabilities = 0.000 0.826 0.174\nPrediction = 1 Actual = 1 Probabilities = 0.210 0.593 0.198\nPrediction = 1 Actual = 2 Probabilities = 0.184 0.639 0.177\nPrediction = 1 Actual = 1 Probabilities = 0.229 0.771 0.000\nPrediction = 0 Actual = 1 Probabilities = 0.442 0.371 0.187\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction = 2 Actual = 1 Probabilities = 0.201 0.397 0.402\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n Overall score = 39.47%\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=5, weights='distance')\n",
    "knn.fit(tr_features, tr_labels)\n",
    "\n",
    "# Look at 10 of our validation set:\n",
    "for i in range(10):\n",
    "    pred = knn.predict([va_features[i]])[0]\n",
    "    probs = knn.predict_proba([va_features[i]])\n",
    "    if n_classes == 5:\n",
    "        print (\"Prediction = %1d Actual = %1d Probabilities = %5.3f %5.3f %5.3f %5.3f %5.3f\" % \n",
    "               (pred, va_labels[i], probs[0][0], probs[0][1], probs[0][2], probs[0][3], probs[0][4]))\n",
    "    if n_classes == 3:\n",
    "        print (\"Prediction = %1d Actual = %1d Probabilities = %5.3f %5.3f %5.3f\" % \n",
    "               (pred, va_labels[i], probs[0][0], probs[0][1], probs[0][2]))\n",
    "\n",
    "# Run the complete validation data set.\n",
    "score = knn.score(va_features, va_labels)\n",
    "print(\"\\n Overall score = %5.2f%%\" % (100.0*score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N =  489\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total wrong = 296. Eror rate =  60.5317%\nClass =  0 False positive = 118 ( 24.1%) False negative = 102 ( 20.9%).\nClass =  1 False positive = 150 ( 30.7%) False negative =  96 ( 19.6%).\nClass =  2 False positive =  28 (  5.7%) False negative =  98 ( 20.0%).\n"
     ]
    }
   ],
   "source": [
    "fp = np.zeros(11)\n",
    "fn = np.zeros(11)\n",
    "n = len(va_labels)\n",
    "tot = 0\n",
    "print(\"N = \", n)\n",
    "for i in range(n):\n",
    "    pred = knn.predict([va_features[i]])[0]\n",
    "    if pred != va_labels[i]:\n",
    "        tot = tot + 1\n",
    "        fn[int(va_labels[i])] = fn[int(va_labels[i])] + 1\n",
    "        fp[int(pred)] = fp[int(pred)] + 1\n",
    "\n",
    "print(\"Total wrong = %3d. Eror rate = %8.4f%%\" % (tot, 100.0*tot/n))\n",
    "for i in range(n_classes):\n",
    "    print(\"Class = %2d False positive = %3d (%5.1f%%) False negative = %3d (%5.1f%%).\" % (i, fp[i], 100.0*fp[i]/n, fn[i\n",
    "    ], 100.0*fn[i]/n))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Build the nested directory structure needed for the retraining approach.\n",
    "if not os.path.exists(\"wines\"):\n",
    "    if n_classes == 5:\n",
    "        os.makedirs(\"wines\")\n",
    "        os.makedirs(\"wines/undrinkable\")\n",
    "        os.makedirs(\"wines/poor\")\n",
    "        os.makedirs(\"wines/ok\")\n",
    "        os.makedirs(\"wines/good\")\n",
    "        os.makedirs(\"wines/excellent\")\n",
    "    if n_classes == 3:\n",
    "        os.makedirs(\"wines\")\n",
    "        os.makedirs(\"wines/undrinkable\")\n",
    "        os.makedirs(\"wines/ok\")\n",
    "        os.makedirs(\"wines/excellent\")\n",
    "    \n",
    "    f_inds = range(11)\n",
    "    for p in range (len(ww_features)):\n",
    "        features = ww_features[p]\n",
    "        for f in f_inds:\n",
    "            features[f] = features[f]/np.average(ww_features[:,f])\n",
    "            \n",
    "        plt.close()\n",
    "        _, bar = plt.subplots()\n",
    "        bar.bar(f_inds, features)\n",
    "        lab = label_map[int(ww_labels[p])]\n",
    "        if lab == 0:\n",
    "            plt.savefig(\"wines/undrinkable/WW%04d.jpeg\"%(p))\n",
    "        if lab == 1:\n",
    "            if n_classes == 5:\n",
    "                plt.savefig(\"wines/poor/WW%04d.jpeg\"%(p))\n",
    "            if n_classes == 3:\n",
    "                plt.savefig(\"wines/ok/WW%04d.jpeg\"%(p))\n",
    "        if lab == 2:\n",
    "            if n_classes == 5:\n",
    "                plt.savefig(\"wines/ok/WW%04d.jpeg\"%(p))\n",
    "            if n_classes == 3:\n",
    "                plt.savefig(\"wines/excellent/WW%04d.jpeg\"%(p))\n",
    "        if lab == 3:\n",
    "            plt.savefig(\"wines/good/WW%04d.jpeg\"%(p))\n",
    "        if lab == 4:\n",
    "            plt.savefig(\"wines/excellent/WW%04d.jpeg\"%(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The retrain.py application is part or the TensorFlow distribution and is discussed in detail in this tutorial: https://www.tensorflow.org/tutorials/image_retraining. This application has many options hat are worth exploring in the source code, but for now we will run it with just the defaults, which we have editied to meet our wines example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not extracting or downloading files, model already present in disk\nModel path:  ./model\\classify_image_graph_def.pb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Looking for images in 'excellent'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Looking for images in 'ok'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Looking for images in 'undrinkable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:500 bottleneck files created.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:1000 bottleneck files created.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:1500 bottleneck files created.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:2000 bottleneck files created.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:2500 bottleneck files created.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:3000 bottleneck files created.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:3500 bottleneck files created.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:4000 bottleneck files created.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:4500 bottleneck files created.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:5000 bottleneck files created.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:5500 bottleneck files created.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:6000 bottleneck files created.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:6500 bottleneck files created.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:7000 bottleneck files created.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:7500 bottleneck files created.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:8000 bottleneck files created.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:8500 bottleneck files created.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:9000 bottleneck files created.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:9500 bottleneck files created.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:2018-02-10 18:40:41.450969: Step 0: Train accuracy = 38.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:2018-02-10 18:40:41.452975: Step 0: Cross entropy = 1.089443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:2018-02-10 18:40:41.642478: Step 0: Validation accuracy = 39.0% (N=100)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:2018-02-10 18:41:14.706454: Step 200: Train accuracy = 47.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:2018-02-10 18:41:14.708461: Step 200: Cross entropy = 1.044380\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:2018-02-10 18:41:14.891947: Step 200: Validation accuracy = 51.0% (N=100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:2018-02-10 18:41:47.483696: Step 400: Train accuracy = 46.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:2018-02-10 18:41:47.485722: Step 400: Cross entropy = 1.017137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:2018-02-10 18:41:47.673187: Step 400: Validation accuracy = 46.0% (N=100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:2018-02-10 18:42:20.657930: Step 600: Train accuracy = 39.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:2018-02-10 18:42:20.659905: Step 600: Cross entropy = 1.035388\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:2018-02-10 18:42:20.841386: Step 600: Validation accuracy = 50.0% (N=100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:2018-02-10 18:42:53.648767: Step 800: Train accuracy = 52.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:2018-02-10 18:42:53.650772: Step 800: Cross entropy = 0.959099\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:2018-02-10 18:42:53.833295: Step 800: Validation accuracy = 46.0% (N=100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:2018-02-10 18:43:26.933471: Step 1000: Train accuracy = 47.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:2018-02-10 18:43:26.935464: Step 1000: Cross entropy = 1.012219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:2018-02-10 18:43:27.133005: Step 1000: Validation accuracy = 46.0% (N=100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:2018-02-10 18:44:00.728746: Step 1200: Train accuracy = 56.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:2018-02-10 18:44:00.729750: Step 1200: Cross entropy = 0.934570\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:2018-02-10 18:44:00.909256: Step 1200: Validation accuracy = 53.0% (N=100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:2018-02-10 18:44:33.816945: Step 1400: Train accuracy = 65.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:2018-02-10 18:44:33.817949: Step 1400: Cross entropy = 0.918402\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:2018-02-10 18:44:34.011498: Step 1400: Validation accuracy = 55.0% (N=100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:2018-02-10 18:45:07.339087: Step 1600: Train accuracy = 62.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:2018-02-10 18:45:07.340121: Step 1600: Cross entropy = 0.901706\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:2018-02-10 18:45:07.533604: Step 1600: Validation accuracy = 50.0% (N=100)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:2018-02-10 18:45:40.369922: Step 1800: Train accuracy = 63.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:2018-02-10 18:45:40.371927: Step 1800: Cross entropy = 0.981387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:2018-02-10 18:45:40.559426: Step 1800: Validation accuracy = 47.0% (N=100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:2018-02-10 18:46:14.413550: Step 2000: Train accuracy = 44.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:2018-02-10 18:46:14.414535: Step 2000: Cross entropy = 0.984128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:2018-02-10 18:46:14.607047: Step 2000: Validation accuracy = 44.0% (N=100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:2018-02-10 18:46:47.464437: Step 2200: Train accuracy = 59.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:2018-02-10 18:46:47.465423: Step 2200: Cross entropy = 0.886935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:2018-02-10 18:46:47.652920: Step 2200: Validation accuracy = 50.0% (N=100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:2018-02-10 18:47:21.386534: Step 2400: Train accuracy = 56.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:2018-02-10 18:47:21.388511: Step 2400: Cross entropy = 0.927718\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:2018-02-10 18:47:21.593052: Step 2400: Validation accuracy = 42.0% (N=100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:2018-02-10 18:47:54.586788: Step 2600: Train accuracy = 56.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:2018-02-10 18:47:54.587794: Step 2600: Cross entropy = 0.946547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:2018-02-10 18:47:54.770278: Step 2600: Validation accuracy = 54.0% (N=100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:2018-02-10 18:48:27.919697: Step 2800: Train accuracy = 61.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:2018-02-10 18:48:27.920730: Step 2800: Cross entropy = 0.906913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:2018-02-10 18:48:28.103183: Step 2800: Validation accuracy = 50.0% (N=100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:2018-02-10 18:49:01.111193: Step 3000: Train accuracy = 58.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:2018-02-10 18:49:01.113233: Step 3000: Cross entropy = 0.918178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:2018-02-10 18:49:01.292677: Step 3000: Validation accuracy = 47.0% (N=100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:2018-02-10 18:49:34.171104: Step 3200: Train accuracy = 58.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:2018-02-10 18:49:34.172108: Step 3200: Cross entropy = 0.888952\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:2018-02-10 18:49:34.359615: Step 3200: Validation accuracy = 48.0% (N=100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:2018-02-10 18:50:10.641360: Step 3400: Train accuracy = 61.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:2018-02-10 18:50:10.642365: Step 3400: Cross entropy = 0.911813\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:2018-02-10 18:50:10.820869: Step 3400: Validation accuracy = 50.0% (N=100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:2018-02-10 18:50:42.770798: Step 3600: Train accuracy = 54.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:2018-02-10 18:50:42.772803: Step 3600: Cross entropy = 0.923449\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:2018-02-10 18:50:42.943257: Step 3600: Validation accuracy = 49.0% (N=100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:2018-02-10 18:51:15.128900: Step 3800: Train accuracy = 53.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:2018-02-10 18:51:15.129935: Step 3800: Cross entropy = 0.903613\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:2018-02-10 18:51:15.298388: Step 3800: Validation accuracy = 54.0% (N=100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:2018-02-10 18:51:46.881336: Step 3999: Train accuracy = 56.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:2018-02-10 18:51:46.882339: Step 3999: Cross entropy = 0.913086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:2018-02-10 18:51:47.054826: Step 3999: Validation accuracy = 55.0% (N=100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Final test accuracy = 50.5% (N=1038)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 2 variables.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 2 variables to const ops.\n"
     ]
    }
   ],
   "source": [
    "%run \"retrain.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
